Codex already has a stdin JSON mechanism, but it‚Äôs for the protocol stream (not a statusline).
  - Existing stdin JSON (JSONL) mode: codex proto
      - Reads JSON lines of codex_core::protocol::Submission from stdin and writes JSON lines of Event to stdout.
      - Code: codex-rs/cli/src/proto.rs; types in codex-rs/protocol/src/protocol.rs.


/status
üìÇ‚ÄäWorkspace
  ‚Ä¢ Path: ~/Documents/DevProjects/codex
  ‚Ä¢ Approval Mode: on-request
  ‚Ä¢ Sandbox: workspace-write
  ‚Ä¢ AGENTS files: AGENTS.md

üë§‚ÄäAccount
  ‚Ä¢ Signed in with ChatGPT
  ‚Ä¢ Login: ouzy.appl.us@gmail.com
  ‚Ä¢ Plan: Team

üß†‚ÄäModel
  ‚Ä¢ Name: gpt-5
  ‚Ä¢ Provider: OpenAI
  ‚Ä¢ Reasoning Effort: Medium
  ‚Ä¢ Reasoning Summaries: Auto

üìä Token Usage
  ‚Ä¢ Session ID: 0a89bcf3-0678-4822-a527-4bf33dbb75ea
  ‚Ä¢ Input: 187862 (+ 2723072 cached)
  ‚Ä¢ Output: 27788
  ‚Ä¢ Total: 215650

Á∫†Ê≠£Ôºö
1. ÈúÄË¶ÅÁÆÄÂçïÊúâÊïàÔºåËé∑ÂæóÁúüÂÆûÁöÑwall time ËÄå‰∏çÊòØÈÄöËøáÊãºÊé•Â§ÑÁêÜÊï∞ÊçÆÔºå‰æãÂ¶ÇÈÄöËøá response Êï∞ÊçÆ‰∏≠Ëé∑Âæó
2. Èô§Ê≠§‰πãÂ§ñÔºåÊúâÂ¶Ç‰∏ã‰ø°ÊÅØÂ∞±ÂèØ‰ª•‰∫Ü
  Workspace.Path: ~/Documents/DevProjects/codex
  Workspace.Approval: on-request
  Workspace.Sandbox: workspace-write
  Model.Name: gpt-5
  Model.Reasoning Effort: Medium

  - Add to every emitted event (proto JSONL):
    - since_session_ms: integer, ms elapsed since the start of this session (monotonic; derive from an Instant).
  - Where to anchor ‚Äústart‚Äù
    - Prefer session start (first SessionConfigured) over process start. It matches what users think of as ‚Äúthis session‚Äôs duration‚Äù and isolates multiple sessions per process.

codebase had use Responeses API by default?

> Yes. The Rust Codex defaults to the Responses API for the built‚Äëin OpenAI provider.

Evidence

- Default provider config: built_in_model_providers() sets wire_api = Responses and (unless overridden) base_url = "https://api.openai.com/v1". Effective path is base_url + "/responses". File: codex-rs/core/src/model_provider_info.rs.
- URL construction: ModelProviderInfo::get_full_url() appends "/responses" when wire_api == Responses; otherwise "/chat/completions". File: codex-rs/core/src/model_provider_info.rs.
- HTTP call: ModelClient::stream() routes to stream_responses for WireApi::Responses and issues a POST with Accept: text/event-stream and header OpenAI-Beta: responses=experimental. File: codex-rs/core/src/client.rs.
- Tests assert POST /v1/responses for OpenAI flows. Files: codex-rs/core/tests/suite/client.rs and codex-rs/core/tests/suite/prompt_caching.rs.

Notes

- You can override the base via OPENAI_BASE_URL or config.toml, but with defaults untouched, OpenAI ‚Üí https://api.openai.com/v1/responses. The ‚Äúoss‚Äù provider intentionally uses Chat Completions.



OK, then move to status display functionality discussion.
you check codebase to confirm how " ‚èé send   ‚åÉJ newline   ‚åÉT transcript   ‚åÉC quit   636669 tokens used   49% context left " to display? we name it as Line1
How to
1. add a new line (Line2) to show " GPT-5 | Medium | Codx ÔΩú workspace-write ÔΩú on-request", "codex" is a Relative path. Line2 data come from exiting codebase and wrote to ccstatus-codex-monitoring.json
2. another new lines (Line3) to display "network probing info" , this will read from ccstatus-codex-monitoring.json file, related data update by a dedicate app.
Requirement
1. add a statusdisplay.rs module and expose API for 3rd data feed in and render display

What's your idea?

  1. Who controls width?
  - Layout/viewport: codex-rs/tui/src/tui.rs computes the viewport width and passes Rects to widgets.
  - Bottom footer (Line 1 today): codex-rs/tui/src/bottom_pane/chat_composer.rs renders the single-line hint/footer into a fixed-height area using Paragraph::new(Line::from(...)). Extra content is clipped by the buffer (i.e., effectively truncated), because no wrapping is applied.
  - Wrapping utilities (available): codex-rs/tui/src/wrapping.rs provides word_wrap_line/word_wrap_lines and RtOptions (initial/subsequent indent, width, breaking). Several widgets already use textwrap or these helpers (e.g., status_indicator_widget uses textwrap::wrap for its queued messages).


